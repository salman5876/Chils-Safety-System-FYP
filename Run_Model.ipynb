{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea1ab60-2f80-45e5-adb2-08ab7bc1a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 435.2ms\n",
      "Speed: 23.5ms preprocess, 435.2ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 327.1ms\n",
      "Speed: 14.2ms preprocess, 327.1ms inference, 14.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 318.1ms\n",
      "Speed: 17.9ms preprocess, 318.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 285.3ms\n",
      "Speed: 16.5ms preprocess, 285.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 347.5ms\n",
      "Speed: 8.4ms preprocess, 347.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 knifes, 389.1ms\n",
      "Speed: 4.8ms preprocess, 389.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 knife, 344.2ms\n",
      "Speed: 0.0ms preprocess, 344.2ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 knife, 333.9ms\n",
      "Speed: 11.4ms preprocess, 333.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 knifes, 331.4ms\n",
      "Speed: 10.9ms preprocess, 331.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 knife, 337.9ms\n",
      "Speed: 12.4ms preprocess, 337.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 329.5ms\n",
      "Speed: 12.3ms preprocess, 329.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 335.1ms\n",
      "Speed: 13.4ms preprocess, 335.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 333.4ms\n",
      "Speed: 11.3ms preprocess, 333.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 329.0ms\n",
      "Speed: 0.0ms preprocess, 329.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 toy, 312.8ms\n",
      "Speed: 0.0ms preprocess, 312.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 toy, 358.2ms\n",
      "Speed: 0.0ms preprocess, 358.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 347.7ms\n",
      "Speed: 14.0ms preprocess, 347.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 344.2ms\n",
      "Speed: 10.3ms preprocess, 344.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 348.6ms\n",
      "Speed: 11.8ms preprocess, 348.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 345.1ms\n",
      "Speed: 4.3ms preprocess, 345.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 331.6ms\n",
      "Speed: 11.1ms preprocess, 331.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 320.0ms\n",
      "Speed: 0.0ms preprocess, 320.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 349.4ms\n",
      "Speed: 0.0ms preprocess, 349.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 343.7ms\n",
      "Speed: 2.9ms preprocess, 343.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 342.6ms\n",
      "Speed: 3.1ms preprocess, 342.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 327.1ms\n",
      "Speed: 0.0ms preprocess, 327.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 336.5ms\n",
      "Speed: 15.0ms preprocess, 336.5ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 378.5ms\n",
      "Speed: 15.0ms preprocess, 378.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 338.7ms\n",
      "Speed: 11.6ms preprocess, 338.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 322.5ms\n",
      "Speed: 10.8ms preprocess, 322.5ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 342.0ms\n",
      "Speed: 0.0ms preprocess, 342.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 318.4ms\n",
      "Speed: 5.4ms preprocess, 318.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 326.5ms\n",
      "Speed: 10.2ms preprocess, 326.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 328.6ms\n",
      "Speed: 0.0ms preprocess, 328.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 362.3ms\n",
      "Speed: 9.0ms preprocess, 362.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 351.4ms\n",
      "Speed: 0.0ms preprocess, 351.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 348.3ms\n",
      "Speed: 0.0ms preprocess, 348.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 355.1ms\n",
      "Speed: 0.0ms preprocess, 355.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 333.9ms\n",
      "Speed: 8.5ms preprocess, 333.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 315.9ms\n",
      "Speed: 0.0ms preprocess, 315.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 367.0ms\n",
      "Speed: 0.0ms preprocess, 367.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 321.6ms\n",
      "Speed: 0.0ms preprocess, 321.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 395.7ms\n",
      "Speed: 0.0ms preprocess, 395.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 341.4ms\n",
      "Speed: 6.5ms preprocess, 341.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 398.9ms\n",
      "Speed: 12.1ms preprocess, 398.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 394.1ms\n",
      "Speed: 3.9ms preprocess, 394.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 334.5ms\n",
      "Speed: 0.0ms preprocess, 334.5ms inference, 11.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 350.7ms\n",
      "Speed: 2.8ms preprocess, 350.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm: Child is too close to an Unsafe Object (knife)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 child, 1 knife, 341.2ms\n",
      "Speed: 4.5ms preprocess, 341.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 360.5ms\n",
      "Speed: 10.5ms preprocess, 360.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 324.1ms\n",
      "Speed: 8.8ms preprocess, 324.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 1 knife, 334.0ms\n",
      "Speed: 0.0ms preprocess, 334.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 360.3ms\n",
      "Speed: 0.0ms preprocess, 360.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 332.9ms\n",
      "Speed: 0.0ms preprocess, 332.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 360.7ms\n",
      "Speed: 0.0ms preprocess, 360.7ms inference, 12.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 child, 366.4ms\n",
      "Speed: 0.0ms preprocess, 366.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "# Open the video file\n",
    "video_path = r\"C:\\Users\\Salman Ahmed\\Desktop\\FYP Defence\\testing videos\\testing4.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Model\n",
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "# Object classes\n",
    "classNames = ['child', 'knife', 'socket', 'toy', 'scissor', 'glass', 'cups', 'ball', 'heater', 'baby_items', 'fork', 'fruit', 'spoon']\n",
    "\n",
    "unsafe_objects = ['knife', 'socket', 'scissor', 'heater', 'fork']\n",
    "\n",
    "safe_objects = ['toy', 'glass', 'cups', 'ball', 'baby_items', 'fruit', 'spoon']\n",
    "\n",
    "# Threshold for triggering an alarm\n",
    "distance_threshold = 500\n",
    "\n",
    "# Frame counter and detection interval\n",
    "frame_counter = 0\n",
    "detection_interval = 10\n",
    "\n",
    "# Function to trigger the alarm\n",
    "def alarm(unsafe_object_name):\n",
    "    print(f\"Alarm: Child is too close to an Unsafe Object ({unsafe_object_name})!\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "    if frame_counter % detection_interval == 0:\n",
    "        results = model(img, stream=True)\n",
    "\n",
    "        # Create lists to store object positions and labels\n",
    "        object_positions = []\n",
    "        object_labels = []\n",
    "\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                confidence = math.ceil((box.conf[0] * 100)) / 100\n",
    "                cls = int(box.cls[0])\n",
    "                label = classNames[cls]\n",
    "\n",
    "                object_positions.append(((x1 + x2) / 2, (y1 + y2) / 2))\n",
    "                object_labels.append(label)\n",
    "\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "                org = [x1, y1]\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                fontScale = 1\n",
    "                color = (255, 0, 0)\n",
    "                thickness = 2\n",
    "                cv2.putText(img, label, org, font, fontScale, color, thickness)\n",
    "\n",
    "        # Check for child and unsafe object proximity and trigger the alarm\n",
    "        for i in range(len(object_positions)):\n",
    "            if object_labels[i] == \"child\":\n",
    "                for j in range(len(object_positions)):\n",
    "                    if object_labels[j] in unsafe_objects:\n",
    "                        distance = math.sqrt((object_positions[i][0] - object_positions[j][0]) ** 2 + (object_positions[i][1] - object_positions[j][1]) ** 2)\n",
    "                        if distance < distance_threshold:\n",
    "                            unsafe_object_name = object_labels[j]\n",
    "                            alarm(unsafe_object_name)\n",
    "\n",
    "    cv2.imshow('Video', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023d907-0125-4222-90b3-8b10ea78bb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
